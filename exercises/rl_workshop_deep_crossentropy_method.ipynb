{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "rl_workshop_deep_crossentropy_method.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8yD6C48D41-",
        "colab_type": "text"
      },
      "source": [
        "# Deep Crossentropy method\n",
        "\n",
        "In this notebook, we will imprement deep crossentropy method to solve the [CartPole-v0 in Open AI Gym](https://gym.openai.com/envs/CartPole-v0)\n",
        "\n",
        "First, we have to make sure we are connected to the right **python 3 reutime and using the GPU**. (Click the 'Runtime' tab and choose 'Change runtime type'), then import the required package (all are already installed in Google Colab)\n",
        "\n",
        "Then run the following 2 cell to install the require library (may take a while) and import them to have our enviroment set up, ignore the warning (it's tricky to display the Open AI Gym videos in Colab notebook)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y2aHJ7esomV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install -U pyglet==1.3.2 > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF6HCYV9HYm5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import os\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSuT3kidsomZ",
        "colab_type": "text"
      },
      "source": [
        "## Let's have a look at the enviroment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_DBUQ2ksomZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"CartPole-v0\").env\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v85vd7XzKgss",
        "colab_type": "text"
      },
      "source": [
        "We can see it's a cart and a pole (as the name suggess). Seems a trival task, but we will use it to demonstarte how we can use neural network as an agent of policy and using deep crosscentropy method.\n",
        "\n",
        "## Task 1: Play the game using MLP\n",
        "\n",
        "Instead of using a array to hold our policy and probability of action to take, we will use a simple MLP neural network to give the probability. The scikit-learn MLPClassifier will do the job."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm-qDDDqsomd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create agent\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "agent = MLPClassifier(hidden_layer_sizes=(50,50),\n",
        "                      activation='tanh',\n",
        "                      warm_start=True, #keep progress between .fit(...) calls\n",
        "                      max_iter=1 #make only 1 iteration on each .fit(...)\n",
        "                     )\n",
        "#initialize agent to the dimension of state an amount of actions\n",
        "agent.fit([env.reset()]*n_actions, list(range(n_actions)));\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCeT33ossomf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_session(t_max=15000):\n",
        "    \n",
        "    states,actions = [],[]\n",
        "    total_reward = 0\n",
        "    \n",
        "    s = env.reset()\n",
        "    \n",
        "    for t in range(t_max):\n",
        "        \n",
        "        # a vector of action probabilities in current state\n",
        "        probs = <your_code> \n",
        "        \n",
        "        a = <your_code>\n",
        "        \n",
        "        new_s, r, done, info = env.step(a)\n",
        "        \n",
        "        #record sessions like you did before\n",
        "        \n",
        "        <your_code>\n",
        "        \n",
        "        s = new_s\n",
        "        if done: break\n",
        "    return states, actions, total_reward\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1Fz1vgJsomh",
        "colab_type": "text"
      },
      "source": [
        "### Deep Crossentropy method steps\n",
        "For the elite selection part, Deep CEM uses exactly the same strategy as the regular CEM.\n",
        "\n",
        "The only difference is that now each observation is not a number but a float32 vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43xMPkj5somi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def select_elites(states_batch,actions_batch,rewards_batch,percentile=50):\n",
        "    \"\"\"\n",
        "    Select states and actions from games that have rewards >= percentile\n",
        "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
        "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
        "    :param rewards_batch: list of rewards, rewards_batch[session_i][t]\n",
        "    \n",
        "    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
        "    \n",
        "    Please return elite states and actions in their original order \n",
        "    [i.e. sorted by session number and timestep within session]\n",
        "    \n",
        "    If you're confused, see examples below. Please don't assume that states are integers (they'll get different later).\n",
        "    \"\"\"\n",
        "    \n",
        "    <your_code>\n",
        "    \n",
        "    return elite_states, elite_actions\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z7cMoq-somk",
        "colab_type": "text"
      },
      "source": [
        "## Here's the training\n",
        "\n",
        "It's the same: Generate sessions, select N best and fit to those."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIa_6PYxsomk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "def show_progress(batch_rewards, log, percentile, reward_range=[-990,+10]):\n",
        "    \"\"\"\n",
        "    A convenience function that displays training progress. \n",
        "    No cool math here, just charts.\n",
        "    \"\"\"\n",
        "    \n",
        "    mean_reward, threshold = np.mean(batch_rewards), np.percentile(batch_rewards, percentile)\n",
        "    log.append([mean_reward, threshold])\n",
        "\n",
        "    clear_output(True)\n",
        "    print(\"mean reward = %.3f, threshold=%.3f\"%(mean_reward, threshold))\n",
        "    plt.figure(figsize=[8,4])\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(list(zip(*log))[0], label='Mean rewards')\n",
        "    plt.plot(list(zip(*log))[1], label='Reward thresholds')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    \n",
        "    plt.subplot(1,2,2)\n",
        "    plt.hist(batch_rewards, range=reward_range);\n",
        "    plt.vlines([np.percentile(batch_rewards, percentile)], [0], [100], label=\"percentile\", color='red')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "115DtlBwT8DV",
        "colab_type": "text"
      },
      "source": [
        "## Task 2: Not update policy?\n",
        "\n",
        "The only difference between DCEM and CEM is that instaed manually updating the policy with propability, we just train the policy agent with the elites."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97zGz9MSsomn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_sessions = 100\n",
        "percentile = 80\n",
        "log = []\n",
        "\n",
        "for i in range(100):\n",
        "    #generate new sessions\n",
        "    sessions = [<your_code>]\n",
        "\n",
        "    batch_states,batch_actions,batch_rewards = map(np.array, zip(*sessions))\n",
        "\n",
        "    elite_states, elite_actions = select_elites(batch_states,batch_actions,batch_rewards,percentile)\n",
        "    \n",
        "    <your_code>\n",
        "\n",
        "    show_progress(batch_rewards, log, percentile)\n",
        "    \n",
        "    if np.mean(batch_rewards) > 190:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66t2fYV9somq",
        "colab_type": "text"
      },
      "source": [
        "## Let's look at results\n",
        "\n",
        "With this enviroment, we can save the session we generate in a mp4 video. Below we show the last one (which is the one achieving the goal, aka 'winning'). You can find all videos by clicking the arrow on the left, choose the `files` tap, inside the `videos` folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l7M32G5somr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#record sessions\n",
        "import gym.wrappers\n",
        "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True)\n",
        "sessions = [generate_session() for _ in range(100)]\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrprDSZgQZRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#play video\n",
        "video_names = list(filter(lambda s:s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
        "mp4 = \"./videos/\"+video_names[-1]\n",
        "video = io.open(mp4, 'r+b').read()\n",
        "encoded = base64.b64encode(video)\n",
        "HTML(data='''<video alt=\"test\" controls style=\"height: 400px;\">\n",
        "            <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "         </video>'''.format(encoded.decode('ascii')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZkAV1Ohsomw",
        "colab_type": "text"
      },
      "source": [
        "## Wanting more? Now what?\n",
        "\n",
        "By this moment you should have got enough score on [CartPole-v0](https://gym.openai.com/envs/CartPole-v0) to consider it solved. It's time to get something harder.\n",
        "\n",
        "* Pick one of environments: [MountainCar-v0](https://gym.openai.com/envs/MountainCar-v0) or [LunarLander-v2](https://gym.openai.com/envs/LunarLander-v2).\n",
        "  * For MountainCar, get average reward of __at least -150__\n",
        "  * For LunarLander, get average reward of __at least +50__\n",
        "\n",
        "See the tips section below, it's kinda important.\n",
        "  \n",
        "  \n",
        "* Bonus quest: Devise a way to speed up training at least 2x against the default version\n",
        "  * Obvious improvement: use [joblib](https://www.google.com/search?client=ubuntu&channel=fs&q=joblib&ie=utf-8&oe=utf-8)\n",
        "  * Try re-using samples from 3-5 last iterations when computing threshold and training\n",
        "  * Experiment with amount of training iterations and learning rate of the neural network (see params)\n",
        "  \n",
        "  \n",
        "### Tips & tricks\n",
        "\n",
        "* Sessions for MountainCar may last for 10k+ ticks. Make sure ```t_max``` param is at least 10k.\n",
        " * Also it may be a good idea to cut rewards via \">\" and not \">=\". If 90% of your sessions get reward of -10k and 20% are better, than if you use percentile 20% as threshold, R >= threshold __fails cut off bad sessions__ whule R > threshold works alright.\n",
        "* _issue with gym_: Some versions of gym limit game time by 200 ticks. This will prevent cem training in most cases. Make sure your agent is able to play for the specified __t_max__, and if it isn't, try `env = gym.make(\"MountainCar-v0\").env` or otherwise get rid of TimeLimit wrapper.\n",
        "* If you use old _swig_ lib for LunarLander-v2, you may get an error. See this [issue](https://github.com/openai/gym/issues/100) for solution.\n",
        "* If it won't train it's a good idea to plot reward distribution and record sessions: they may give you some clue.\n",
        "* 20-neuron network is probably not enough, feel free to experiment."
      ]
    }
  ]
}