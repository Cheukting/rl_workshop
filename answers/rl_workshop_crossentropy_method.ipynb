{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "rl_workshop_crossentropy_method.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BxBk84bp4wC",
        "colab_type": "text"
      },
      "source": [
        "# Crossentropy method\n",
        "\n",
        "In this notebook, we will imprement crossentropy method to solve the ['Taxi' problem in Open AI Gym](https://gym.openai.com/envs/Taxi-v2/)\n",
        "\n",
        "First, we have to make sure we are connected to the right **python 3 reutime and using the GPU**. (Click the 'Runtime' tab and choose 'Change runtime type'), then import the required package (all are already installed in Google Colab)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4ucVW4Qp4wD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np, pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7adMV64Et-r",
        "colab_type": "text"
      },
      "source": [
        "Then we create the 'Taxi' enviroment from the Gym. Render the initial state to check it is imported properly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPiSeYf5Erqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make(\"Taxi-v2\")\n",
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl3wtYs_GIje",
        "colab_type": "text"
      },
      "source": [
        "Find out how many states and action we can have in this enviroment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kcwaSI7p4wI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_states = env.observation_space.n\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "print(\"n_states=%i, n_actions=%i\"%(n_states, n_actions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DogESWKmp4wL",
        "colab_type": "text"
      },
      "source": [
        "## Task 1: Create stochastic policy\n",
        "\n",
        "Let's create a policy, for crossentrupy method with stochastic policy (updated each epoch), it will be a probability distribution of taking action a will result in state s.\n",
        "\n",
        "```policy[s,a] = P(take action a | in state s)```\n",
        "\n",
        "It could be represented in a 2-D array, and we want to initialize it __uniformly__. In another word, at the beginning state,probability of choosing all actions should be equal (and adding up to 1).\n",
        "\n",
        "With `n_state` and `n_action`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79LDIEUmp4wM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "policy = np.ones((n_states,n_actions)) * (1/n_actions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-RRIPR3p4wO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert type(policy) in (np.ndarray,np.matrix)\n",
        "assert np.allclose(policy,1./n_actions)\n",
        "assert np.allclose(np.sum(policy,axis=1), 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtB4cVZJp4wQ",
        "colab_type": "text"
      },
      "source": [
        "## Task 2: Play the game\n",
        "\n",
        "Let's play the game with our policy. The following function will 'play the game' i.e. picking actions according to the state we are in and following the policy given. It will keep on 'playing' till the game is over or the step (t) reached the maximum limit. (To avoid endless loop) The action and states will be reconded and returned. Also the performance of the policy (sum of rewards) will also be returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQdj_dHLp4wQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_session(policy,t_max=10**4):\n",
        "    \"\"\"\n",
        "    Play game until end or for t_max ticks.\n",
        "    :param policy: an array of shape [n_states,n_actions] with action probabilities\n",
        "    :returns: list of states, list of actions and sum of rewards\n",
        "    \"\"\"\n",
        "    states,actions = [],[]\n",
        "    total_reward = 0.\n",
        "    \n",
        "    s = env.reset()\n",
        "    \n",
        "    for t in range(t_max):\n",
        "        \n",
        "        a = np.random.choice(n_actions,1,p=policy[s,:])[0]\n",
        "        \n",
        "        new_s, r, done, info = env.step(a)\n",
        "        \n",
        "        #Record state, action and add up reward to states,actions and total_reward accordingly. \n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        total_reward += r\n",
        "        \n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "    return states, actions, total_reward\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OAc-QDyp4wS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s,a,r = generate_session(policy)\n",
        "assert type(s) == type(a) == list\n",
        "assert len(s) == len(a)\n",
        "assert type(r) in [float,np.float]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLet-FrI-E1C",
        "colab_type": "text"
      },
      "source": [
        "## Let's see the initial reward distribution\n",
        "\n",
        "In the following cell we play the game 200 times using the function `generate_session` we implemented above and plot to examine the performance of our policy..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCqRGN36p4wU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "sample_rewards = [generate_session(policy,t_max=1000)[-1] for _ in range(200)]\n",
        "\n",
        "plt.hist(sample_rewards,bins=20);\n",
        "plt.vlines([np.percentile(sample_rewards, 50)], [0], [100], label=\"50'th percentile\", color='green')\n",
        "plt.vlines([np.percentile(sample_rewards, 90)], [0], [100], label=\"90'th percentile\", color='red')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYBfD-5t-tS6",
        "colab_type": "text"
      },
      "source": [
        "You can see that it is not that great :-( So we now use crossentropy method to improve 'train' a better policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgEK8Fsnp4wX",
        "colab_type": "text"
      },
      "source": [
        "## Task 3: Crossentropy method steps\n",
        "\n",
        "The goal is to select the state-action pairs which perform better than a certain percentile (elite_states, elites_actions). So we know what is a better choice actions in a certain state. You can say it's trail and error, and yes, that is the basic of reinforcement learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0jacA8Ep4wY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def select_elites(states_batch,actions_batch,rewards_batch,percentile=50):\n",
        "    \"\"\"\n",
        "    Select states and actions from games that have rewards >= percentile\n",
        "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
        "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
        "    :param rewards_batch: list of rewards, rewards_batch[session_i][t]\n",
        "    \n",
        "    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
        "    \n",
        "    Please return elite states and actions in their original order \n",
        "    [i.e. sorted by session number and timestep within session]\n",
        "    \n",
        "    If you're confused, see examples below. Please don't assume that states are integers (they'll get different later).\n",
        "    \"\"\"\n",
        "    \n",
        "    def gen_list(input_list, output_list):\n",
        "        for indx,val in enumerate(input_list):\n",
        "            if rewards_batch[indx] >= reward_threshold:\n",
        "                if isinstance(val, list):\n",
        "                    for i in range(len(val)):\n",
        "                        output_list.append(val[i])\n",
        "                else:\n",
        "                    output_list.append(val)\n",
        "    \n",
        "    reward_threshold = np.percentile(rewards_batch,percentile)\n",
        "    \n",
        "    elite_states  = []   \n",
        "    elite_actions = []\n",
        "    \n",
        "    gen_list(states_batch,elite_states)\n",
        "    gen_list(actions_batch,elite_actions)\n",
        "    \n",
        "    return elite_states,elite_actions\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puefOHVFp4wa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states_batch = [\n",
        "    [1,2,3],   #game1\n",
        "    [4,2,0,2], #game2\n",
        "    [3,1]      #game3\n",
        "]\n",
        "\n",
        "actions_batch = [\n",
        "    [0,2,4],   #game1\n",
        "    [3,2,0,1], #game2\n",
        "    [3,3]      #game3\n",
        "]\n",
        "rewards_batch = [\n",
        "    3,         #game1\n",
        "    4,         #game2\n",
        "    5,         #game3\n",
        "]\n",
        "\n",
        "test_result_0 = select_elites(states_batch, actions_batch, rewards_batch, percentile=0)\n",
        "test_result_40 = select_elites(states_batch, actions_batch, rewards_batch, percentile=30)\n",
        "test_result_90 = select_elites(states_batch, actions_batch, rewards_batch, percentile=90)\n",
        "test_result_100 = select_elites(states_batch, actions_batch, rewards_batch, percentile=100)\n",
        "\n",
        "assert np.all(test_result_0[0] == [1, 2, 3, 4, 2, 0, 2, 3, 1])  \\\n",
        "   and np.all(test_result_0[1] == [0, 2, 4, 3, 2, 0, 1, 3, 3]),\\\n",
        "        \"For percentile 0 you should return all states and actions in chronological order\"\n",
        "assert np.all(test_result_40[0] == [4, 2, 0, 2, 3, 1]) and \\\n",
        "        np.all(test_result_40[1] ==[3, 2, 0, 1, 3, 3]),\\\n",
        "        \"For percentile 30 you should only select states/actions from two first\"\n",
        "assert np.all(test_result_90[0] == [3,1]) and \\\n",
        "        np.all(test_result_90[1] == [3,3]),\\\n",
        "        \"For percentile 90 you should only select states/actions from one game\"\n",
        "assert np.all(test_result_100[0] == [3,1]) and\\\n",
        "       np.all(test_result_100[1] == [3,3]),\\\n",
        "        \"Please make sure you use >=, not >. Also double-check how you compute percentile.\"\n",
        "print(\"Ok!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm6nZp6cAQEd",
        "colab_type": "text"
      },
      "source": [
        "## Task 4: Update policy\n",
        "\n",
        "Now we have the 'elites' list, we can update our policy so the probability of the state-action pairs are propotional to their occurance of in the 'elites' list. In other words, the state-action which seems to have a higher chance to perform better get selected more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRMzX-7Lp4we",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_policy(elite_states,elite_actions):\n",
        "    \"\"\"\n",
        "    Given old policy and a list of elite states/actions from select_elites,\n",
        "    return new updated policy where each action probability is proportional to\n",
        "    \n",
        "    policy[s_i,a_i] ~ #[occurences of si and ai in elite states/actions]\n",
        "    \n",
        "    Don't forget to normalize policy to get valid probabilities and handle 0/0 case.\n",
        "    In case you never visited a state, set probabilities for all actions to 1./n_actions\n",
        "    \n",
        "    :param elite_states: 1D list of states from elite sessions\n",
        "    :param elite_actions: 1D list of actions from elite sessions\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    new_policy = np.zeros([n_states,n_actions])\n",
        "    esp = 0.0000001\n",
        "    \n",
        "    for i,s in enumerate(elite_states):\n",
        "        new_policy[s,elite_actions[i]] += 1\n",
        "        \n",
        "    for s in range(n_states):\n",
        "        if all(new_policy[s,:] == 0):\n",
        "            new_policy[s,:] = 1./n_actions\n",
        "        else:\n",
        "            new_policy[s,:] /= np.sum(new_policy[s,:])\n",
        "            \n",
        "    #Don't forget to set 1/n_actions for all actions in unvisited states.\n",
        "    \n",
        "    \n",
        "    return new_policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mumQNLK6p4wg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "elite_states, elite_actions = ([1, 2, 3, 4, 2, 0, 2, 3, 1], [0, 2, 4, 3, 2, 0, 1, 3, 3])\n",
        "\n",
        "\n",
        "new_policy = update_policy(elite_states,elite_actions)\n",
        "\n",
        "assert np.isfinite(new_policy).all(), \"Your new policy contains NaNs or +-inf. Make sure you don't divide by zero.\"\n",
        "assert np.all(new_policy>=0), \"Your new policy can't have negative action probabilities\"\n",
        "assert np.allclose(new_policy.sum(axis=-1),1), \"Your new policy should be a valid probability distribution over actions\"\n",
        "reference_answer = np.array([\n",
        "       [ 1.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
        "       [ 0.5       ,  0.        ,  0.        ,  0.5       ,  0.        ],\n",
        "       [ 0.        ,  0.33333333,  0.66666667,  0.        ,  0.        ],\n",
        "       [ 0.        ,  0.        ,  0.        ,  0.5       ,  0.5       ]])\n",
        "assert np.allclose(new_policy[:4,:5],reference_answer)\n",
        "print(\"Ok!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCK1uSLTp4wj",
        "colab_type": "text"
      },
      "source": [
        "## Let's train it\n",
        "Let's put all our work above together: Generate sessions, select N best and fit to those. (Double check you are using the GPU runtime type, training may take a while, be patient)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJVx8Ifpp4wk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "def show_progress(batch_rewards, log, percentile, reward_range=[-990,+10]):\n",
        "    \"\"\"\n",
        "    A convenience function that displays training progress. \n",
        "    No cool math here, just charts.\n",
        "    \"\"\"\n",
        "    \n",
        "    mean_reward, threshold = np.mean(batch_rewards), np.percentile(batch_rewards, percentile)\n",
        "    log.append([mean_reward,threshold])\n",
        "\n",
        "    clear_output(True)\n",
        "    print(\"mean reward = %.3f, threshold=%.3f\"%(mean_reward, threshold))\n",
        "    plt.figure(figsize=[8,4])\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(list(zip(*log))[0], label='Mean rewards')\n",
        "    plt.plot(list(zip(*log))[1], label='Reward thresholds')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    \n",
        "    plt.subplot(1,2,2)\n",
        "    plt.hist(batch_rewards,range=reward_range);\n",
        "    plt.vlines([np.percentile(batch_rewards, percentile)], [0], [100], label=\"percentile\", color='red')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmoj_BaJp4wo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reset policy just in case\n",
        "policy = np.ones([n_states, n_actions]) / n_actions "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT5piqEyp4wq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_sessions = 250  #sample this many sessions\n",
        "percentile = 50  #take this percent of session with highest rewards\n",
        "learning_rate = 0.5  #add this thing to all counts for stability\n",
        "\n",
        "log = []\n",
        "\n",
        "for i in range(100):\n",
        "    \n",
        "    %time sessions = [generate_session(policy) for i in range(n_sessions)]\n",
        "    \n",
        "    batch_states,batch_actions,batch_rewards = zip(*sessions)\n",
        "\n",
        "    elite_states, elite_actions = select_elites(batch_states,batch_actions,batch_rewards,percentile)\n",
        "    \n",
        "    new_policy = update_policy(elite_states,elite_actions)\n",
        "    \n",
        "    policy = learning_rate * new_policy + (1-learning_rate) * policy\n",
        "    \n",
        "    #display results on chart\n",
        "    show_progress(batch_rewards, log, percentile)\n",
        "    if np.mean(batch_rewards) > -20:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmQcJYjAp4wt",
        "colab_type": "text"
      },
      "source": [
        "## Footnote: Reflecting on results\n",
        "\n",
        "You may have noticed that the taxi problem quickly converges from <-1000 to a near-optimal score and then descends back into -50/-100. This is in part because the environment has some innate randomness. Namely, the starting points of passenger/driver change from episode to episode.\n",
        "\n",
        "In case CEM failed to learn how to win from one distinct starting point, it will siply discard it because no sessions from that starting point will make it into the \"elites\".\n",
        "\n",
        "To mitigate that problem, you can either reduce the threshold for elite sessions (duct tape way) or  change the way you evaluate strategy (theoretically correct way). You can first sample an action for every possible state and then evaluate this choice of actions by running _several_ games and averaging rewards."
      ]
    }
  ]
}